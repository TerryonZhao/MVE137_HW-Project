{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42d4e64",
   "metadata": {},
   "source": [
    "**Project Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325c54a",
   "metadata": {},
   "source": [
    "**Formalities**\n",
    "\n",
    "This is the project for the course Probability and Statistical Learning Using Python, 2024. Here, you are asked to carry out the analysis using the tools, techniques, and skills acquired in the course and hand in a .pynb file with the solutions.\n",
    "\n",
    "The **deadline  is Friday, November 01, 2024.** You should upload the solution file to 'Project Assignment' in Canvas via 'Home-->Project Assignment'.\n",
    "Note that this is an individual exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab58442",
   "metadata": {},
   "source": [
    "**Part I**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a735f",
   "metadata": {},
   "source": [
    "In this exercise we will estimate the test error of logistic regression model using the below described validation set approach. You will neeed to import the *Default.csv* file provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679f0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b68f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "#Uncomment and set the path to the csv file below\n",
    "df = pd.read_csv(\"Default.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6928055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>820.017113</td>\n",
       "      <td>51584.65732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>619.751869</td>\n",
       "      <td>15750.62208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1047.718124</td>\n",
       "      <td>46416.97099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>243.841328</td>\n",
       "      <td>47193.88813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>186.500387</td>\n",
       "      <td>45430.55027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    default student      balance       income\n",
       "1        No      No   729.526495  44361.62507\n",
       "2        No     Yes   817.180407  12106.13470\n",
       "3        No      No  1073.549164  31767.13895\n",
       "4        No      No   529.250605  35704.49394\n",
       "5        No      No   785.655883  38463.49588\n",
       "..      ...     ...          ...          ...\n",
       "96       No      No   820.017113  51584.65732\n",
       "97       No     Yes   619.751869  15750.62208\n",
       "98       No      No  1047.718124  46416.97099\n",
       "99       No      No   243.841328  47193.88813\n",
       "100      No      No   186.500387  45430.55027\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722b0a63-c6c8-4456-be99-a4475e580378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>820.017113</td>\n",
       "      <td>51584.65732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>619.751869</td>\n",
       "      <td>15750.62208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1047.718124</td>\n",
       "      <td>46416.97099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243.841328</td>\n",
       "      <td>47193.88813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186.500387</td>\n",
       "      <td>45430.55027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     default  student      balance       income\n",
       "1          0        0   729.526495  44361.62507\n",
       "2          0        1   817.180407  12106.13470\n",
       "3          0        0  1073.549164  31767.13895\n",
       "4          0        0   529.250605  35704.49394\n",
       "5          0        0   785.655883  38463.49588\n",
       "..       ...      ...          ...          ...\n",
       "96         0        0   820.017113  51584.65732\n",
       "97         0        1   619.751869  15750.62208\n",
       "98         0        0  1047.718124  46416.97099\n",
       "99         0        0   243.841328  47193.88813\n",
       "100        0        0   186.500387  45430.55027\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['default'] = df['default'].map({'Yes': 1, 'No': 0})\n",
    "df['student'] = df['student'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e4c6d",
   "metadata": {},
   "source": [
    "(a) i.) Fit a logistic regression model that uses $income$ and $balance$ to\n",
    "predict $default$ and *print out the summary*. ii.) Next, Implement k-fold cross-validation with k=5 for the logistic regression model to ensure the model's performance is stable across different data splits. Print the cross-validation score for each fold and its average cross validation accuracy. **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786cea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- i.)) --------------------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "Summary of Logistic Regression\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Tue, 15 Oct 2024   Pseudo R-squ.:                  0.4594\n",
      "Time:                        14:13:57   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "--------------------- ii.)) --------------------\n",
      "Fold  0 : accuracy =  0.9745\n",
      "Fold  1 : accuracy =  0.9735\n",
      "Fold  2 : accuracy =  0.9665\n",
      "Fold  3 : accuracy =  0.9655\n",
      "Fold  4 : accuracy =  0.975\n",
      "Average accuracy =  0.9709999999999999\n"
     ]
    }
   ],
   "source": [
    "##### i.)\n",
    "print('--------------------- i.)) --------------------')\n",
    "\n",
    "# Define x and y\n",
    "x = df[['income', 'balance']]\n",
    "y = df['default']\n",
    "\n",
    "# Apply model from smf\n",
    "model_logit = smf.logit('default ~ income + balance', df).fit()\n",
    "\n",
    "print('Summary of Logistic Regression')\n",
    "print(model_logit.summary())\n",
    "\n",
    "##### ii.)\n",
    "print('--------------------- ii.)) --------------------')\n",
    "\n",
    "\n",
    "# Split training and test set\n",
    "x_train,  x_test,  y_train,  y_test  =  train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "# Apply model from sklearn\n",
    "logit = LogisticRegression()\n",
    "logit.fit(x_train, y_train)\n",
    "\n",
    "y_pred = logit.predict(x_test)\n",
    "\n",
    "# K-fold\n",
    "accuracy = cross_val_score(logit, x, y, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "\n",
    "# Print\n",
    "for i in range(len(accuracy)):\n",
    "    print('Fold ', i, ': accuracy = ', accuracy[i])\n",
    "                           \n",
    "average_accuracy = np.mean(accuracy)\n",
    "print('Average accuracy = ', average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302291b",
   "metadata": {},
   "source": [
    "(b) You are supposed to estimate the test error of this model using the validation set approach described below. In order to do this, you must perform the following steps: **(4 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31129a5",
   "metadata": {},
   "source": [
    "i. Split the sample set into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fff91e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test set\n",
    "x_train,  x_val,  y_train,  y_val  =  train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41deb6",
   "metadata": {},
   "source": [
    "ii. Fit a multiple logistic regression model using only the training\n",
    "observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52306566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression()\n",
    "logit.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e13126",
   "metadata": {},
   "source": [
    "iii. Obtain a prediction of default status for each individual in\n",
    "the validation set (test set) by computing the posterior probability of\n",
    "$default$ for that individual, and classifying the individual to\n",
    "the $default$ category if the posterior probability is greater\n",
    "than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2e4618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted default:  [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = logit.predict_proba(x_test)\n",
    "y_pred_prob = y_pred_prob[:,0]\n",
    "\n",
    "y_pred = np.zeros(len(y_pred_prob))\n",
    "\n",
    "# Compare\n",
    "for i in range(len(y_pred_prob)):\n",
    "    if y_pred_prob[i] > 0.5:\n",
    "        y_pred [i] = 0\n",
    "    else:\n",
    "        y_pred [i] = 1\n",
    "\n",
    "print('Predicted default: ', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bfa3b",
   "metadata": {},
   "source": [
    "iv. Compute the validation set error, which is the fraction of\n",
    "the observations in the validation set (test set) that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a636670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9536666666666667\n",
      "Test error:  0.04633333333333334\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = accuracy_score(y_val, y_pred)\n",
    "test_error = 1 - val_accuracy\n",
    "\n",
    "print('Accuracy: ', val_accuracy)\n",
    "print('Test error: ', test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f375ea",
   "metadata": {},
   "source": [
    "(c) Now consider a logistic regression model that predicts the probability of default using $income$, $balance$, a dummy variable for $student$ and print the summary. Estimate the test error for this model using the validation\n",
    "set approach. Comment on the results. Does the inclusion of a dummy variable for student lead to a reduction in the test error? **(3 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737e28e-f6b9-40a5-8227-fec3e87d837e",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "#### Test Error\n",
    "- When the variable \"student\" is not included, test error = 0.028;\n",
    "- When the variable \"student\" is included, test errror = 0.0357.\n",
    "\n",
    "#### Conclusion:\n",
    "The inclusion of a dummy variable for student *DOES NOT* lead to a reduction in the test error. Oppositly, it add to the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0e54900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------NOT include variable Student: ------------------------------\n",
      "Accuracy:  0.972\n",
      "Test Error:  0.028000000000000025\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n",
      "Summary:                             Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9997\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Tue, 15 Oct 2024   Pseudo R-squ.:                  0.4594\n",
      "Time:                        14:54:48   Log-Likelihood:                -789.48\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                4.541e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n",
      "-------------------------- Include variable Student: ------------------------------\n",
      "Accuracy:  0.9643333333333334\n",
      "Test Error:  0.035666666666666624\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078577\n",
      "         Iterations 10\n",
      "Summary:                             Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                          Logit   Df Residuals:                     9996\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 15 Oct 2024   Pseudo R-squ.:                  0.4619\n",
      "Time:                        14:54:48   Log-Likelihood:                -785.77\n",
      "converged:                       True   LL-Null:                       -1460.3\n",
      "Covariance Type:            nonrobust   LLR p-value:                3.257e-292\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -10.8690      0.492    -22.079      0.000     -11.834      -9.904\n",
      "income      3.033e-06    8.2e-06      0.370      0.712    -1.3e-05    1.91e-05\n",
      "balance        0.0057      0.000     24.737      0.000       0.005       0.006\n",
      "student       -0.6468      0.236     -2.738      0.006      -1.110      -0.184\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.15 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "### NOT include student\n",
    "x_1 = df[['income', 'balance']]\n",
    "y_1 = df['default']\n",
    "\n",
    "x_1_train, x_1_test, y_1_train, y_1_test = train_test_split(x_1, y_1, test_size=0.3)\n",
    "\n",
    "logit_1 = LogisticRegression()\n",
    "logit_1.fit(x_1_train, y_1_train)\n",
    "\n",
    "y_1_pred = logit_1.predict(x_1_test)\n",
    "\n",
    "print('--------------------------NOT include variable Student: ------------------------------')\n",
    "print('Accuracy: ', accuracy_score(y_1_test, y_1_pred))\n",
    "print(\"Test Error: \", 1 - accuracy_score(y_1_test, y_1_pred))\n",
    "\n",
    "logit_summary_1 = smf.logit('default ~ income + balance', data=df).fit().summary()\n",
    "print(\"Summary: \", logit_summary_1)\n",
    "\n",
    "\n",
    "### Include student\n",
    "x_2 = df[['income', 'balance','student']]\n",
    "y_2 = df['default']\n",
    "\n",
    "x_2_train, x_2_test, y_2_train, y_2_test = train_test_split(x_2, y_2, test_size=0.3)\n",
    "\n",
    "logit_2 = LogisticRegression()\n",
    "logit_2.fit(x_2_train, y_2_train)\n",
    "\n",
    "y_2_pred = logit_2.predict(x_2_test)\n",
    "\n",
    "print('-------------------------- Include variable Student: ------------------------------')\n",
    "print('Accuracy: ', accuracy_score(y_2_test, y_2_pred))\n",
    "print(\"Test Error: \", 1 - accuracy_score(y_2_test, y_2_pred))\n",
    "\n",
    "logit_summary_2 = smf.logit('default ~ income + balance + student', data=df).fit().summary()\n",
    "print(\"Summary: \", logit_summary_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f6b09",
   "metadata": {},
   "source": [
    "**Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eb2b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f0854",
   "metadata": {},
   "source": [
    "In this exercise, you will demonstrate your understanding of the KNN classification algorithm and test it on a breast cancer dataset. The algorithm should be implemented in pure python, without using the sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ce805",
   "metadata": {},
   "source": [
    "**KNN algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048b3fc",
   "metadata": {},
   "source": [
    "(a)  Implement a function of your own to perform KNN classification **without using the default available libraries such as KNeighborsClassifier() in sklearn**. You will need to consider the Euclidean distances between the features and data to be predicted (test data) when selecting the k-nearest neighbors. The function should take 3 inputs as 1) data set to train the model 2) data to test 3) number of neighbours (k). If *k* is set to a value less than or equal to the total classification groups, the function should give a warning, and  warn() function defined in the 'warning' module will be useful for that.\n",
    "\n",
    "The function should output *classification_result*, where *classification_result* is the result of your classifier. Further, you should provide a suitable measure of the confidence on the classification. Justify your choice. \n",
    "\n",
    "Please note that only a few basic libraries/modules are imported and thus you are expected to import the needed others. \n",
    "\n",
    "\n",
    "\n",
    "**Hint:** You may fill and complete the function given below.                **(5 pts)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d468242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_algorithm(traindata, testdata, k_neighbours=5):\n",
    "    \n",
    "    # 1. Check if k is proper\n",
    "    class_num = len(traindata.iloc[:, -1].unique())\n",
    "    sample_num = len(traindata)\n",
    "    \n",
    "    if k_neighbours <= class_num:\n",
    "        warnings.warn('K is less than or equal to class number!')\n",
    "    if k_neighbours > sample_num:\n",
    "        warnings.warn('K is larger than sample number!')\n",
    "    \n",
    "    \n",
    "    # 2. Get features(x) and labels(y) from dataset\n",
    "    x_train = traindata.iloc[:, :-1].values\n",
    "    y_train = traindata.iloc[:, -1].values\n",
    "    x_test = testdata.iloc[:, :-1].values\n",
    "    #y_test = testdata.iloc[:, -1].values      # which is the correct label\n",
    "    \n",
    "    # 3. Initial output array\n",
    "    classification_result = []\n",
    "    confidence = []\n",
    "    \n",
    "    # 4. Campare Euclidean distances between test data and EACH train data\n",
    "    for test_sample in x_test:\n",
    "        distance = []\n",
    "        for train_sample in x_train:\n",
    "            \n",
    "            E_distance = np.sqrt(np.sum((test_sample - train_sample)**2))\n",
    "            distance.append(E_distance)\n",
    "            \n",
    "        # 5. Find the nearest K neighbours\n",
    "        idx_smalltolarge = np.argsort(distance)\n",
    "        idx_k_nearest = idx_smalltolarge[:k_neighbours]\n",
    "        \n",
    "        label_k_nearest = y_train[idx_k_nearest]\n",
    "    \n",
    "        # 6. Vote for the label of test point by neighbours' labels\n",
    "        label_count = np.bincount(label_k_nearest)    # value: occur time; idx: label\n",
    "        most_common_label = label_count.argmax()      # output idx(label)\n",
    "    \n",
    "        # 7. Record classification results and Confidence\n",
    "        classification_result.append(most_common_label)\n",
    "        # 7.1 Calculate confidence\n",
    "        count = label_count[most_common_label]\n",
    "        confidence_i = count / k_neighbours\n",
    "        confidence.append(confidence_i)\n",
    "\n",
    "    # Return\n",
    "    return classification_result, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b1202",
   "metadata": {},
   "source": [
    "#### Now let's test the implemented KNN algorithm on the given breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd0f63",
   "metadata": {},
   "source": [
    "This dataset contains records of breast cancer patients. Here we will use the features (columns) to predict the correct cancer class (last column) for the patients in the dataset as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27af2716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                         int64\n",
      "clump_thickness            int64\n",
      "unif_cell_size             int64\n",
      "unif_cell_shape            int64\n",
      "marg_adhesion              int64\n",
      "single_epith_cell_size     int64\n",
      "bare_nuclei               object\n",
      "bland_chrom                int64\n",
      "norm_nucleoli              int64\n",
      "mitoses                    int64\n",
      "class                      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "columns = ['id', 'clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion',\n",
    "           'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses', 'class']\n",
    "#Import the data file by uncommenting below and setting the path to the dataset\n",
    "df = pd.read_csv('breast-cancer-wisconsin.data', header=None, names=columns)\n",
    "df.head(500)\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb999d6b",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be7ad9",
   "metadata": {},
   "source": [
    "(b) Check how many different cancer classes are available, and plot a pie chart to see the distribution of classes. Then, find and replace all the missing values with the mode of the particular column(s). Note that the missing values are marked with '?' in the dataset. **(1 pt)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4883d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtypes_df：\n",
      "id                         int64\n",
      "clump_thickness            int64\n",
      "unif_cell_size             int64\n",
      "unif_cell_shape            int64\n",
      "marg_adhesion              int64\n",
      "single_epith_cell_size     int64\n",
      "bare_nuclei               object\n",
      "bland_chrom                int64\n",
      "norm_nucleoli              int64\n",
      "mitoses                    int64\n",
      "class                      int64\n",
      "dtype: object\n",
      "['1' '10' '2' '4' '3' '9' '7' '5' '8' '6']\n",
      "[ 1 10  2  4  3  9  7  5  8  6]\n",
      "id                        int64\n",
      "clump_thickness           int64\n",
      "unif_cell_size            int64\n",
      "unif_cell_shape           int64\n",
      "marg_adhesion             int64\n",
      "single_epith_cell_size    int64\n",
      "bare_nuclei               int64\n",
      "bland_chrom               int64\n",
      "norm_nucleoli             int64\n",
      "mitoses                   int64\n",
      "class                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "class_num = df['class'].nunique()\n",
    "print(\"Number of classes: \", class_num)\n",
    "\n",
    "class_series = df['class'].value_counts()\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(class_series, labels=class_series.index, autopct='%1.1f%%', startangle=90, colors=['lightskyblue','gold'])\n",
    "plt.title('Class Distribution')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# According to the print above, all missing values are in \"bare_nuclei\" column\n",
    "mode_value = df['bare_nuclei'].mode()[0]\n",
    "df['bare_nuclei'].replace(to_replace='?', value=mode_value, inplace=True)\n",
    "\n",
    "print(\"dtypes_df：\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check the unique values\n",
    "print(df['bare_nuclei'].unique())\n",
    "\n",
    "# Convert 'str' to 'int'\n",
    "df['bare_nuclei'] = pd.to_numeric(df['bare_nuclei'], errors='coerce')\n",
    "print(df['bare_nuclei'].unique())\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80206cd",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a7b6c",
   "metadata": {},
   "source": [
    "(c) Drop the obiviously unwanted column(s). **(1 pt)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67debe30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>unif_cell_size</th>\n",
       "      <th>unif_cell_shape</th>\n",
       "      <th>marg_adhesion</th>\n",
       "      <th>single_epith_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chrom</th>\n",
       "      <th>norm_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     clump_thickness  unif_cell_size  unif_cell_shape  marg_adhesion  \\\n",
       "0                  5               1                1              1   \n",
       "1                  5               4                4              5   \n",
       "2                  3               1                1              1   \n",
       "3                  6               8                8              1   \n",
       "4                  4               1                1              3   \n",
       "..               ...             ...              ...            ...   \n",
       "495                3               1                1              1   \n",
       "496                1               1                1              1   \n",
       "497                4               2                1              1   \n",
       "498                4               1                1              1   \n",
       "499                4               1                1              1   \n",
       "\n",
       "     single_epith_cell_size  bare_nuclei  bland_chrom  norm_nucleoli  mitoses  \\\n",
       "0                         2            1            3              1        1   \n",
       "1                         7           10            3              2        1   \n",
       "2                         2            2            3              1        1   \n",
       "3                         3            4            3              7        1   \n",
       "4                         2            1            3              1        1   \n",
       "..                      ...          ...          ...            ...      ...   \n",
       "495                       1            1            2              1        1   \n",
       "496                       1            1            1              1        1   \n",
       "497                       2            1            1              1        1   \n",
       "498                       2            1            2              1        1   \n",
       "499                       2            1            2              1        1   \n",
       "\n",
       "     class  \n",
       "0        2  \n",
       "1        2  \n",
       "2        2  \n",
       "3        2  \n",
       "4        2  \n",
       "..     ...  \n",
       "495      2  \n",
       "496      2  \n",
       "497      2  \n",
       "498      2  \n",
       "499      2  \n",
       "\n",
       "[500 rows x 10 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('id', axis=1)\n",
    "df.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f60fc",
   "metadata": {},
   "source": [
    "#### Split the training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7067dc",
   "metadata": {},
   "source": [
    "(d) In here, you will split the data into training and test sets, **without using sklearn library.** Please use 20% of the dataset as test set and the rest for train set. Shuffle the data prior to splitting in order to prevent any bias during the training and to avoid the model from learning the order of the training. **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e09a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>unif_cell_size</th>\n",
       "      <th>unif_cell_shape</th>\n",
       "      <th>marg_adhesion</th>\n",
       "      <th>single_epith_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chrom</th>\n",
       "      <th>norm_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clump_thickness  unif_cell_size  unif_cell_shape  marg_adhesion  \\\n",
       "0                5               1                1              6   \n",
       "1                1               1                1              1   \n",
       "2                4               1                1              2   \n",
       "3                4               2                3              5   \n",
       "4                3               1                1              1   \n",
       "\n",
       "   single_epith_cell_size  bare_nuclei  bland_chrom  norm_nucleoli  mitoses  \\\n",
       "0                       3            1            1              1        1   \n",
       "1                       2            1            2              1        1   \n",
       "2                       2            1            1              1        1   \n",
       "3                       3            8            7              6        1   \n",
       "4                       3            1            2              1        1   \n",
       "\n",
       "   class  \n",
       "0      2  \n",
       "1      2  \n",
       "2      2  \n",
       "3      4  \n",
       "4      2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle\n",
    "df_shuffled = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "df_shuffled.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "195c8421-ab5d-4ea8-8ecc-61695bc2d3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set:  (559, 10)\n",
      "SHape of test set： (140, 10)\n",
      "Propotion of train set =  0.7997138769670958\n",
      "dtypes_train：\n",
      "clump_thickness           int64\n",
      "unif_cell_size            int64\n",
      "unif_cell_shape           int64\n",
      "marg_adhesion             int64\n",
      "single_epith_cell_size    int64\n",
      "bare_nuclei               int64\n",
      "bland_chrom               int64\n",
      "norm_nucleoli             int64\n",
      "mitoses                   int64\n",
      "class                     int64\n",
      "dtype: object\n",
      "dtypes_test：\n",
      "clump_thickness           int64\n",
      "unif_cell_size            int64\n",
      "unif_cell_shape           int64\n",
      "marg_adhesion             int64\n",
      "single_epith_cell_size    int64\n",
      "bare_nuclei               int64\n",
      "bland_chrom               int64\n",
      "norm_nucleoli             int64\n",
      "mitoses                   int64\n",
      "class                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "train_size = int(0.8 * len(df_shuffled))\n",
    "traindata = df_shuffled.iloc[:train_size]\n",
    "testdata = df_shuffled.iloc[train_size:]\n",
    "\n",
    "# Check\n",
    "num_train = len(traindata)\n",
    "num_test = len(testdata)\n",
    "propotion_train = (num_train)/(num_test + num_train)\n",
    "\n",
    "# Print\n",
    "print('Shape of train set: ', traindata.shape)\n",
    "print(\"SHape of test set：\", testdata.shape)\n",
    "print('Propotion of train set = ', propotion_train)\n",
    "\n",
    "# Check the dtype before using knn-model\n",
    "print(\"dtypes_train：\")\n",
    "print(traindata.dtypes)\n",
    "\n",
    "print(\"dtypes_test：\")\n",
    "print(testdata.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eda4a2",
   "metadata": {},
   "source": [
    "#### Predict the class for test data and calculate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173dff6",
   "metadata": {},
   "source": [
    "(e) Now train the KNN classifier you developed in (a) using the training set, and test it on the test set with *k_neighbours=5*. **(4 pts)**\n",
    "\n",
    "Print the confidence for the incorrect predictions the classifier has made.\n",
    "\n",
    "Find the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bad73b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Incorrect Predictions --------------------\n",
      "Serial Number 26: Predict Label 2, Real Label 4, Confidence 0.8\n",
      "Serial Number 62: Predict Label 2, Real Label 4, Confidence 0.8\n",
      "Serial Number 72: Predict Label 2, Real Label 4, Confidence 0.6\n",
      "Serial Number 82: Predict Label 4, Real Label 2, Confidence 1.0\n",
      "Serial Number 84: Predict Label 2, Real Label 4, Confidence 0.6\n",
      "Serial Number 91: Predict Label 2, Real Label 4, Confidence 0.8\n",
      "Serial Number 111: Predict Label 4, Real Label 2, Confidence 0.6\n",
      "Serial Number 124: Predict Label 4, Real Label 2, Confidence 1.0\n",
      "-------------------- Accuracy --------------------\n",
      "Accuracy =  0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "results, confs = knn_algorithm(traindata, testdata, k_neighbours=5)\n",
    "\n",
    "error = 0\n",
    "\n",
    "print('-------------------- Incorrect Predictions --------------------')\n",
    "for i in range(len(results)):\n",
    "    if results[i] != testdata.iloc[i, -1]:\n",
    "        print(f\"Serial Number {i}: Predict Label {results[i]}, Real Label {testdata.iloc[i, -1]}, Confidence {confs[i]}\")\n",
    "        error += 1\n",
    "\n",
    "        \n",
    "print('-------------------- Accuracy --------------------')    \n",
    "accuracy = 1 - error/len(testdata)\n",
    "print('Accuracy = ', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f358925",
   "metadata": {},
   "source": [
    "**Effect of reduction in training data size on confidence of predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5137829",
   "metadata": {},
   "source": [
    "(f) Now increase the test set size to 40% of the dataset while keeping the same *k_neighbours* and print the confidence and accuracy of the predictions similar to the previous question. Explain the results in comparison with (e) **(2 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad9f89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set:  (419, 10)\n",
      "SHape of test set： (280, 10)\n",
      "Propotion of train set =  0.5994277539341917\n",
      "dtypes_train：\n",
      "clump_thickness           int64\n",
      "unif_cell_size            int64\n",
      "unif_cell_shape           int64\n",
      "marg_adhesion             int64\n",
      "single_epith_cell_size    int64\n",
      "bare_nuclei               int64\n",
      "bland_chrom               int64\n",
      "norm_nucleoli             int64\n",
      "mitoses                   int64\n",
      "class                     int64\n",
      "dtype: object\n",
      "dtypes_test：\n",
      "clump_thickness           int64\n",
      "unif_cell_size            int64\n",
      "unif_cell_shape           int64\n",
      "marg_adhesion             int64\n",
      "single_epith_cell_size    int64\n",
      "bare_nuclei               int64\n",
      "bland_chrom               int64\n",
      "norm_nucleoli             int64\n",
      "mitoses                   int64\n",
      "class                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Shuffle\n",
    "df_shuffled = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "df_shuffled.head(5)\n",
    "\n",
    "# Split\n",
    "train_size = int(0.6 * len(df_shuffled))\n",
    "traindata = df_shuffled.iloc[:train_size]\n",
    "testdata = df_shuffled.iloc[train_size:]\n",
    "\n",
    "# Check\n",
    "num_train = len(traindata)\n",
    "num_test = len(testdata)\n",
    "propotion_train = (num_train)/(num_test + num_train)\n",
    "\n",
    "# Print\n",
    "print('Shape of train set: ', traindata.shape)\n",
    "print(\"SHape of test set：\", testdata.shape)\n",
    "print('Propotion of train set = ', propotion_train)\n",
    "\n",
    "# Check the dtype before using knn-model\n",
    "print(\"dtypes_train：\")\n",
    "print(traindata.dtypes)\n",
    "\n",
    "print(\"dtypes_test：\")\n",
    "print(testdata.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e1e2214-4439-41b6-b621-999b8fc46535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Incorrect Predictions --------------------\n",
      "Serial Number 106: Predict Label 2, Real Label 4, Confidence 1.0\n",
      "Serial Number 115: Predict Label 2, Real Label 4, Confidence 0.6\n",
      "Serial Number 129: Predict Label 4, Real Label 2, Confidence 0.8\n",
      "Serial Number 166: Predict Label 2, Real Label 4, Confidence 0.8\n",
      "Serial Number 178: Predict Label 2, Real Label 4, Confidence 0.6\n",
      "Serial Number 202: Predict Label 2, Real Label 4, Confidence 0.6\n",
      "Serial Number 212: Predict Label 2, Real Label 4, Confidence 0.6\n",
      "Serial Number 222: Predict Label 4, Real Label 2, Confidence 1.0\n",
      "Serial Number 231: Predict Label 2, Real Label 4, Confidence 0.8\n",
      "Serial Number 251: Predict Label 4, Real Label 2, Confidence 0.8\n",
      "Serial Number 264: Predict Label 4, Real Label 2, Confidence 1.0\n",
      "-------------------- Accuracy --------------------\n",
      "Accuracy =  0.9607142857142857\n"
     ]
    }
   ],
   "source": [
    "results, confs = knn_algorithm(traindata, testdata, k_neighbours=5)\n",
    "\n",
    "error = 0\n",
    "\n",
    "print('-------------------- Incorrect Predictions --------------------')\n",
    "for i in range(len(results)):\n",
    "    if results[i] != testdata.iloc[i, -1]:\n",
    "        print(f\"Serial Number {i}: Predict Label {results[i]}, Real Label {testdata.iloc[i, -1]}, Confidence {confs[i]}\")\n",
    "        error += 1\n",
    "\n",
    "        \n",
    "print('-------------------- Accuracy --------------------')    \n",
    "accuracy = 1 - error/len(testdata)\n",
    "print('Accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb1406-1a7b-4512-9c08-0dbe94e687fd",
   "metadata": {},
   "source": [
    "#### Results in Camparison\n",
    "\n",
    "- When $\\text{Propotion of train set} \\approx 0.80$, The $ \\text{Accuracy} \\approx  0.94$\n",
    "- When $\\text{Propotion of train set} \\approx 0.60$, The $ \\text{Accuracy} \\approx  0.96$\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "##### 1. Less impact of overfit\n",
    "\n",
    "Althogh less traindata may reduce the bias of the model in every sample, at the same time it also add to the variance(Bias-Variance Trade-off):\n",
    "\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "\\operatorname{Err}(\\boldsymbol{x})& =\\mathsf{Var}[\\mathbf{y}|\\boldsymbol{x}]+(\\tilde{f}(\\boldsymbol{x})-\\mathbb{E}_{\\mathcal{D}}[f(\\boldsymbol{x})])^{2}+\\mathbb{E}_{\\mathcal{D}}\\left[(\\mathbb{E}_{\\mathcal{D}}[f(\\boldsymbol{x})]-f(\\boldsymbol{x}))^{2}\\right] \\\\\n",
    "&=\\sigma^2+(\\underbrace{\\tilde{f}(\\boldsymbol{x})-\\mathbb{E}_{\\mathcal{D}}[f(\\boldsymbol{x})]}_{\\mathrm{bias}})^2+\\underbrace{\\mathsf{Var}_{\\mathcal{D}}\\left[f(\\boldsymbol{x})\\right]}_{\\mathrm{variance}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "There may also be risk of overfit considering the large-propotion traindata. Therefore, less traindata will reduce the impact of overfit and allow the model to have a better anti-noise preformance.\n",
    "\n",
    "##### 2. The model is sensitive to local data.\n",
    "\n",
    "The reduction of train data may remove some local data which is far away from other points so that the nearest neighbours can be those which can indeed represent the features of the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a84cc8",
   "metadata": {},
   "source": [
    "#### Alternative classification methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c8ec6",
   "metadata": {},
   "source": [
    "(g) Propose alternative classification approaches for this problem and discuss the advantages and disadvantages with respect to KNNs (you don't need to implement them). **(2 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214efcdb-757d-439b-9039-3129eecb698e",
   "metadata": {},
   "source": [
    "It is a *binary classification problem*, so **Logistic Regression** and **Desicion Tree** may also works.\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "##### Advantage:\n",
    "\n",
    "1. Simplicity:\n",
    "\n",
    "    Logistic regression is better than KNN in terms of traing time, especialy when the data is large.\n",
    "\n",
    "2. Good performance for linear separable data and high-dimension data：\n",
    "    \n",
    "    When the data can be separated by a linear boundary, logistic regression performs very well. What's more, it is suitable for high-dimension data while KNN may have the problem of \"curse of dimensionality\" in high-dimensional space.\n",
    "\n",
    "##### Drawback:\n",
    "\n",
    "1. Limit performance for non-linear separable data:\n",
    "\n",
    "    When the decision boundary of the data is nonlinear, logistic regression cannot effectively classify, while KNN can handle data with nonlinear boundaries more flexibly because it is based on distance measurement.\n",
    "\n",
    "#### Desicion Tree\n",
    "\n",
    "##### Advantage:\n",
    "\n",
    "1. No need for feature scaling: \n",
    "\n",
    "    Decision trees do not rely on feature metrics, so there is no need to standardize or normalize the data like in logistic regression or KNN.\n",
    "\n",
    "##### Drawback:\n",
    "\n",
    "1. Easy to be overfitting.\n",
    "\n",
    "    Desicion tree tends to create very specific splits based on the training data. Overfitting occurs when a model becomes too complex, capturing noise or random fluctuations in the training data instead of the underlying patterns.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
